{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Sense Disambiguation ToSubmit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8YaE4jOygUe"
      },
      "source": [
        "### Downloading data for the task, some additonal corpora and functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hrWy-pl2NIa"
      },
      "source": [
        "Downloading all necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBRNATxJ2Ppi"
      },
      "source": [
        "import sys\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pandas import read_csv\n",
        "from evaluate import evaluate\n",
        "\n",
        "import gensim\n",
        "import logging\n",
        "from sklearn.cluster import AffinityPropagation, SpectralClustering, KMeans\n",
        "import requests\n",
        "from pymystem3 import Mystem"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZnUOAo72U1o"
      },
      "source": [
        "Downloading data for the task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5CXikIwykT8",
        "outputId": "ee587bf5-ddf2-4200-bb60-9f20a48e4eca"
      },
      "source": [
        "!git clone https://github.com/nlpub/russe-wsi-kit.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'russe-wsi-kit'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 148 (delta 4), reused 22 (delta 4), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (148/148), 3.83 MiB | 8.87 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY4-02Tj2mCg"
      },
      "source": [
        "sys.path.append('/content/russe-wsi-kit/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3wQ2noyxmK"
      },
      "source": [
        "Test data for the wiki-wiki dataset is changed manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRyhdcclzCzx"
      },
      "source": [
        "Getting Natasha module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1Z6DboOy29n",
        "outputId": "b77b1883-7436-40bb-965d-6ebd9569ee40"
      },
      "source": [
        "! pip install navec"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting navec\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec) (1.19.5)\n",
            "Installing collected packages: navec\n",
            "Successfully installed navec-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xccfRzWzI-c",
        "outputId": "de9a9b61-e551-4df4-9112-544a233e10e1"
      },
      "source": [
        "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-28 14:21:51--  https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53012480 (51M) [application/x-tar]\n",
            "Saving to: ‘navec_hudlit_v1_12B_500K_300d_100q.tar.1’\n",
            "\n",
            "navec_hudlit_v1_12B 100%[===================>]  50.56M  16.2MB/s    in 3.1s    \n",
            "\n",
            "2021-11-28 14:21:55 (16.2 MB/s) - ‘navec_hudlit_v1_12B_500K_300d_100q.tar.1’ saved [53012480/53012480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgYqyNYrzQh_"
      },
      "source": [
        "from navec import Navec\n",
        "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "navec = Navec.load(path)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnxLX1AUzSbf"
      },
      "source": [
        "Getting fasttext module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXcMhcwkzVss",
        "outputId": "23c42617-21e7-460f-b45c-4f241176220c"
      },
      "source": [
        "!wget http://vectors.nlpl.eu/repository/20/214.zip"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-28 17:50:05--  http://vectors.nlpl.eu/repository/20/214.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1920218982 (1.8G) [application/zip]\n",
            "Saving to: ‘214.zip’\n",
            "\n",
            "214.zip             100%[===================>]   1.79G  24.9MB/s    in 75s     \n",
            "\n",
            "2021-11-28 17:51:20 (24.4 MB/s) - ‘214.zip’ saved [1920218982/1920218982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RE6kt9p26y5",
        "outputId": "a4aa4856-8474-489f-e1a3-df288f8abe87"
      },
      "source": [
        "!unzip 214.zip -d ru_fasttext_model"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  214.zip\n",
            "  inflating: ru_fasttext_model/meta.json  \n",
            "  inflating: ru_fasttext_model/model.model  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_ngrams.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_vocab.npy  \n",
            "  inflating: ru_fasttext_model/README  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzTK1K7a3Cb_"
      },
      "source": [
        "! Additionaly corporas were downloaded from https://rusvectores.org/en/models/ manually and saved at drive/My Drive/Corpora/\n",
        "\n",
        "Corporas that I tried are \n",
        "\n",
        "1) ruscorpora_upos_skipgram_300_5_2018.vec.gz\n",
        "\n",
        "2) taiga_upos_skipgram_300_2_2018.vec.gz\n",
        "\n",
        "3) ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz\n",
        "\n",
        "4) news_upos_cbow_600_2_2018.vec.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_36j1-F_2uwM"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHU18OOX4rFD"
      },
      "source": [
        "Tokenization, lemmatization, splitting, lowering and position tagging (needed for the word2vec models taken from rusvectors website) is made with the help of the function \n",
        "tag_mystem.\n",
        "\n",
        "Originally the functuion was taken from here and simplified/adapted https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_mystem.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q15zdLex5ayc"
      },
      "source": [
        "def tag_mystem(\n",
        "    text=\"Текст нужно передать функции в виде строки!\", mapping=None):\n",
        "  \n",
        "    m = Mystem()\n",
        "    processed = m.analyze(text)\n",
        "    tagged = []\n",
        "\n",
        "    for w in processed:\n",
        "      if 'analysis' in w and len(w['analysis']) > 0: \n",
        "        lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
        "        pos = w[\"analysis\"][0][\"gr\"].split(\",\")[0]\n",
        "        pos = pos.split(\"=\")[0].strip()\n",
        "        if mapping:\n",
        "            if pos in mapping:\n",
        "                pos = mapping[pos]  \n",
        "            else:\n",
        "                pos = \"X\"  #на случай, если попадется тэг, которого нет в маппинге\n",
        "        tagged.append(lemma.lower() + \"_\" + pos)\n",
        "    return tagged"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dagSJjtI6cGx"
      },
      "source": [
        "Функция tag_mystem на выход выдает морфомлогические тэги в формате Mystem, чтобы преобразовать их в тэги UPoS, которые принимает бльшое число используемых мной корпусов, необходим следующий mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ifj7BcX6FhS"
      },
      "source": [
        "mapping_url = \"https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map\"\n",
        "\n",
        "mystem2upos = {}\n",
        "r = requests.get(mapping_url, stream=True)\n",
        "for pair in r.text.split(\"\\n\"):\n",
        "    pair = pair.split()\n",
        "    if len(pair) > 1:\n",
        "        mystem2upos[pair[0]] = pair[1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kApwopHZ6qLI"
      },
      "source": [
        "Пример преобразования изначального датасета с помощью функции tag_mystem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M8vCoH66ptM"
      },
      "source": [
        "train_raw = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train.csv\", sep='\\t')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwVYbWZI2wkW"
      },
      "source": [
        "#Transformation of the field 'Word'\n",
        "df_part = pd.DataFrame()\n",
        "a = train_raw['word'].unique()\n",
        "for i in range(len(a)):\n",
        "  df_part.loc[i, 'word'] = a[i]\n",
        "  df_part.loc[i, 'word_new'] = a[i]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgpM5gUP7agT"
      },
      "source": [
        "Code needed to run Mystem in Colab correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq9TN44b7ZXI",
        "outputId": "2d1af0f2-63db-4992-f7d1-c8ac36a1fc9c"
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-28 14:23:42--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.244, 5.45.205.243, 5.45.205.242, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.244|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-man01i.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
            "--2021-11-28 14:23:43--  http://cache-man01i.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving cache-man01i.cdn.yandex.net (cache-man01i.cdn.yandex.net)... 5.45.205.221, 2a02:6b8::3:221\n",
            "Connecting to cache-man01i.cdn.yandex.net (cache-man01i.cdn.yandex.net)|5.45.205.221|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  9.28MB/s    in 1.7s    \n",
            "\n",
            "2021-11-28 14:23:45 (9.28 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxfMBxw37BYN",
        "outputId": "3d7d0fd1-359a-4858-df45-6f5977edea64"
      },
      "source": [
        "%%time\n",
        "for i in range(len(df_part)):\n",
        "  new = tag_mystem(df_part.word.loc[i], mapping=mystem2upos)\n",
        "  df_part.word_new.loc[i] = new[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.7 ms, sys: 90.2 ms, total: 117 ms\n",
            "Wall time: 3.77 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPKVxSqj76_J"
      },
      "source": [
        "new = pd.merge(train_raw, df_part, on='word')\n",
        "new.drop('word',1, inplace= True)\n",
        "new.rename(columns={\"word_new\": \"word\"}, inplace = True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppFEXTj27-cn",
        "outputId": "d0e29c8a-a009-4bb3-dbde-d4bf48919dd5"
      },
      "source": [
        "#Transformation of the 'Context' field\n",
        "%%time\n",
        "for i in range(len(new)):\n",
        "  context = tag_mystem(new.context.loc[i], mapping=mystem2upos)\n",
        "  new.context.loc[i] = ' '.join(context)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.95 s, sys: 9.72 s, total: 12.7 s\n",
            "Wall time: 6min 58s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "618UggF28Ulj"
      },
      "source": [
        "new.to_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_tagged.csv\", index=False, sep='\\t')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx8FitVp8fFe"
      },
      "source": [
        "Actually, the time for the large datasets such as active-dict takes approximately 1.5 hour to run. So I ran it in other places and saved. The preprocessed datasets can be found in attached folder. Further I asssume that they are situated in the respective folders in Colab. \n",
        "\n",
        "Like for wiki-wiki dataset, it must be saved by the following path: /russe-wsi-kit/data/main/wiki-wiki/\n",
        "\n",
        "The files are named for each datset in the following manner: train_wiki-wiki_tagged.csv, test_wiki-wiki_tagged.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut-EtPfcHchv"
      },
      "source": [
        "For some corpora I need data in the format witoug morphology tags. So to transforms datasets above like train_wiki-wiki_tagged.csv I will use the transformation below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obPOjNOrN7Ob"
      },
      "source": [
        "Wiki-wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGzYX-b1IHXP",
        "outputId": "8f519a7d-b28c-488b-ca9f-fff95c13943a"
      },
      "source": [
        "df_tags = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_tagged.csv\", sep='\\t')\n",
        "for i in range(len(df_tags)):\n",
        "    target = df_tags.word.loc[i].split('_')[0]\n",
        "    df_tags.word.loc[i] = target\n",
        "for i in range(len(df_tags)):\n",
        "  target = df_tags.context.loc[i]\n",
        "  words = target.split()\n",
        "  words = [word.split('_')[0] for word in words]\n",
        "  newcontext = ' '.join(words)\n",
        "  df_tags.context.loc[i] = newcontext\n",
        "df_tags.to_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_notags.csv\", index=False, sep='\\t')"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1fQYpIN_wt"
      },
      "source": [
        "Bts-rnc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yjUOsY3N8z_",
        "outputId": "6be1d0ca-d3cd-49d7-b0db-f541cdfe398c"
      },
      "source": [
        "df_tags = pd.read_csv(\"./russe-wsi-kit/data/main/bts-rnc/train_bts-rnc_tagged.csv\", sep='\\t')\n",
        "for i in range(len(df_tags)):\n",
        "    target = df_tags.word.loc[i].split('_')[0]\n",
        "    df_tags.word.loc[i] = target\n",
        "for i in range(len(df_tags)):\n",
        "  target = df_tags.context.loc[i]\n",
        "  words = target.split()\n",
        "  words = [word.split('_')[0] for word in words]\n",
        "  newcontext = ' '.join(words)\n",
        "  df_tags.context.loc[i] = newcontext\n",
        "df_notags.to_csv(\"./russe-wsi-kit/data/main/bts-rnc/train_bts-rnc_notags.csv\", index=False, sep='\\t')"
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXEcbXv3ONT3"
      },
      "source": [
        "Active-dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj3yuLw0N-ls",
        "outputId": "6b3c2210-67ec-41a5-bc4f-16c5bedf8027"
      },
      "source": [
        "df_tags3 = pd.read_csv(\"./russe-wsi-kit/data/main/active-dict/train_active-dict_tagged.csv\", sep='\\t')\n",
        "for i in range(len(df_tags)):\n",
        "    target = df_tags.word.loc[i].split('_')[0]\n",
        "    df_tags.word.loc[i] = target\n",
        "for i in range(len(df_tags)):\n",
        "  target = df_tags.context.loc[i]\n",
        "  words = target.split()\n",
        "  words = [word.split('_')[0] for word in words]\n",
        "  newcontext = ' '.join(words)\n",
        "  df_tags.context.loc[i] = newcontext\n",
        "df_notags.to_csv(\"./russe-wsi-kit/data/main/active-dict/train_active-dict_notags.csv\", index=False, sep='\\t')"
      ],
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFq1V3QrNzLU"
      },
      "source": [
        "Further I also save to russe-wsi-kit/data/main corresponding datasets and use them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aePA5yW--Wm"
      },
      "source": [
        "### Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSFDyKEu_CEi"
      },
      "source": [
        "1) After text preprocessing I firstly take the baseline solution of this contest from here https://github.com/akutuzov/russian_wsi (actually, it was quaite challenging for me to start it running)\n",
        "\n",
        "The solution is as follows\n",
        "\n",
        "- In each cluster of words (here I mean, for instance, all meanings of word 'замок') we take the vector embeddings of each word of the context with the function fingerprint() and take the mean of the sum of the vectors.\n",
        "\n",
        "- The resulting vectors are then ran through two step-clustering by the function predict_clusters(): first, with Affinity Propagation to evaluate the amount of clusters and after that the eastimated amount of clusters is used in Spectral Clustering.\n",
        "\n",
        "2) The upgrade of the baseline solution is \n",
        "\n",
        "- trying different corpora, including those without position tagging \n",
        "- trying different clustering hyperparametes + changing at the second step Spectral Clustering to KMeans Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVRqIveB9Uq6"
      },
      "source": [
        "def fingerprint_pos(text, model):\n",
        "    \"\"\"\n",
        "    :param text: list of words\n",
        "    :param model: word2vec model in Gensim format\n",
        "    :return: average vector of words in text\n",
        "    \"\"\"\n",
        "    # Creating list of all words in the document, which are present in the model\n",
        "    #words = [w for w in text if w in model]\n",
        "    words = [w for w in text if w in model]\n",
        "    lexicon = list(set(words))\n",
        "    lw = len(lexicon)\n",
        "    if lw < 1:\n",
        "        print('Empty lexicon in', text, file=sys.stderr)\n",
        "        return np.zeros(model.vector_size)\n",
        "    vectors = np.zeros((lw, model.vector_size))  # Creating empty matrix of vectors for words\n",
        "    for i in list(range(lw)):  # Iterate over words in the text\n",
        "        word = lexicon[i]\n",
        "        vectors[i, :] = model[word] \n",
        "    semantic_fingerprint = np.sum(vectors, axis=0)  # Computing sum of all vectors in the document\n",
        "    semantic_fingerprint = np.divide(semantic_fingerprint, lw)  # Computing average vector\n",
        "    return semantic_fingerprint\n",
        "\n",
        "def fingerprint_nopos(text, model):\n",
        "    \"\"\"\n",
        "    :param text: list of words\n",
        "    :param model: word2vec model in Gensim format\n",
        "    :return: average vector of words in text\n",
        "    \"\"\"\n",
        "    words = [w for w in text if w in model.vocab]\n",
        "    length = 300\n",
        "    lexicon = list(set(words))\n",
        "    lw = len(lexicon)\n",
        "    if lw < 1:\n",
        "        print('Empty lexicon in', text, file=sys.stderr)\n",
        "        return np.zeros(300)\n",
        "    vectors = np.zeros((len(words), 300))  # Creating empty matrix of vectors for words\n",
        "    for i in range(len(words)):  # Iterate over words in the text\n",
        "        word =  words[i]\n",
        "        vectors[i, :] = model[word] \n",
        "    semantic_fingerprint = np.sum(vectors, axis=0)  # Computing sum of all vectors in the document\n",
        "    semantic_fingerprint = np.divide(semantic_fingerprint, len(words))  # Computing average vector\n",
        "    return semantic_fingerprint"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqvUrbMTF3UP"
      },
      "source": [
        "def predict_clusters_pos(model, df, damping, preference, ninit):\n",
        "  predicted = []\n",
        "  goldsenses = []\n",
        "  for query in df.word.unique():\n",
        "      print('Now analyzing', query, '...', file=sys.stderr)\n",
        "      subset = df[df.word == query]\n",
        "      contexts = []\n",
        "      matrix = np.empty((subset.shape[0], model.vector_size))\n",
        "      counter = 0\n",
        "      lengths = []\n",
        "      for line in subset.iterrows():\n",
        "          con = line[1].context\n",
        "          identifier = line[1].context_id\n",
        "          label = query + str(identifier)\n",
        "          contexts.append(label)\n",
        "          if type(con) == float:\n",
        "              print('Empty context at', label, file=sys.stderr)\n",
        "              fp = np.zeros(model.vector_size)\n",
        "          else:\n",
        "              bow = con.split()\n",
        "              bow = [b for b in bow if b != query]\n",
        "              fp = fingerprint_pos(bow, model)\n",
        "              lengths.append(len(bow))\n",
        "          matrix[counter, :] = fp\n",
        "          counter += 1\n",
        "      clustering = AffinityPropagation(damping=damping, preference = preference,\n",
        "                                      random_state=None).fit(matrix)\n",
        "\n",
        "      nclusters = len(clustering.cluster_centers_indices_)\n",
        "      if nclusters < 1:\n",
        "          print('Fallback to 1 cluster!', file=sys.stderr)\n",
        "          nclusters = 1\n",
        "      elif nclusters == len(contexts):\n",
        "          print('Fallback to 4 clusters!', file=sys.stderr)\n",
        "          nclusters = 4\n",
        "      clustering = SpectralClustering(n_clusters=nclusters, n_init= ninit, \n",
        "                                    assign_labels='discretize', n_jobs=2).fit(matrix)\n",
        "      #clustering = KMeans(n_clusters=nclusters, \n",
        "       #                  random_state=None).fit(matrix)\n",
        "\n",
        "    # End two-stage clustering\n",
        "      cur_predicted = clustering.labels_.tolist()\n",
        "      predicted += cur_predicted\n",
        "      print('Predicted clusters:', len(set(cur_predicted)), file=sys.stderr)\n",
        "\n",
        "  df.predict_sense_id = predicted\n",
        "\n",
        "  return df\n",
        "\n",
        "def predict_clusters_nopos(model, df, damping, preference):\n",
        "  predicted = []\n",
        "  goldsenses = []\n",
        "  for query in df.word.unique():\n",
        "      print('Now analyzing', query, '...', file=sys.stderr)\n",
        "      subset = df[df.word == query]\n",
        "      contexts = []\n",
        "      matrix = np.empty((subset.shape[0], 300))\n",
        "      counter = 0\n",
        "      lengths = []\n",
        "      for line in subset.iterrows():\n",
        "          con = line[1].context\n",
        "          identifier = line[1].context_id\n",
        "          label = query + str(identifier)\n",
        "          contexts.append(label)\n",
        "          if type(con) == float:\n",
        "              print('Empty context at', label, file=sys.stderr)\n",
        "              fp = np.zeros(model.vector_size)\n",
        "          else:\n",
        "              bow = con.split()\n",
        "              bow = [b for b in bow if b != query]\n",
        "              fp = fingerprint_nopos(bow, model)\n",
        "              lengths.append(len(bow))\n",
        "          matrix[counter, :] = fp\n",
        "          counter += 1\n",
        "      clustering = AffinityPropagation(damping=damping, preference = preference,\n",
        "                                      random_state=None).fit(matrix)\n",
        "\n",
        "      nclusters = len(clustering.cluster_centers_indices_)\n",
        "      if nclusters < 1:\n",
        "          print('Fallback to 1 cluster!', file=sys.stderr)\n",
        "          nclusters = 1\n",
        "      elif nclusters == len(contexts):\n",
        "          print('Fallback to 4 clusters!', file=sys.stderr)\n",
        "          nclusters = 4\n",
        "      clustering = SpectralClustering(n_clusters=nclusters, n_init=25,\n",
        "                                    assign_labels='discretize', n_jobs=2).fit(matrix)\n",
        "      #clustering = KMeans(n_clusters=nclusters, \n",
        "      #                   random_state=None).fit(matrix)\n",
        "\n",
        "    # End two-stage clustering\n",
        "      cur_predicted = clustering.labels_.tolist()\n",
        "      predicted += cur_predicted\n",
        "      print('Predicted clusters:', len(set(cur_predicted)), file=sys.stderr)\n",
        "\n",
        "  df.predict_sense_id = predicted\n",
        "\n",
        "  return df"
      ],
      "execution_count": 641,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDhJ2PMlG6t7"
      },
      "source": [
        "### Training phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWDBH-WKL033"
      },
      "source": [
        "How to run the solution?\n",
        "\n",
        "1) Choose corpora for train (if from block №1 uncomment one of the corporas and model, if from block №2 choose just one of the models)\n",
        "\n",
        "2) Choose parameters for the first step of clustering (damping, preference)\n",
        "\n",
        "3) Choose dataset with tags or with no tags \n",
        "- in case you use chose the model from block 1, then choose dataset with tags\n",
        "- in case you use chose the model from block 2, then choose dataset with no tags\n",
        "\n",
        "4) Choose function predict_clusters_pos() if you use corpora from block1, choose predict_clusters_pos() if you use corpora from block2\n",
        "\n",
        "5) If you want to change the method of clustering at the second step, it should be done manually - in either function choose predict_clusters_pos() or in function choose predict_clusters_nopos() comment/uncomment rows with Spectral Clustering/KMeans\n",
        "\n",
        "Great! You are beautiful! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5opqvxoKMeGv"
      },
      "source": [
        "Step 1. Corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwSBvinM8eRV"
      },
      "source": [
        "#1\n",
        "modelfile = \"drive/My Drive/Corpora/ruscorpora_upos_skipgram_300_5_2018.vec.gz\"\n",
        "#modelfile = \"drive/My Drive/Corpora/taiga_upos_skipgram_300_2_2018.vec.gz\"\n",
        "#modelfile = \"drive/My Drive/Corpora/ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz\"\n",
        "#modelfile = \"drive/My Drive/Corpora/news_upos_cbow_600_2_2018.vec.gz\"\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(modelfile, binary=False)\n",
        "\n",
        "#2\n",
        "#model = gensim.models.KeyedVectors.load('ru_fasttext_model/model.model')\n",
        "#model = navec"
      ],
      "execution_count": 503,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ7Gm8ADHCEm"
      },
      "source": [
        "Step 2. Parameters to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFCdcf_C8ce3"
      },
      "source": [
        "ninit = 25\n",
        "damping = 0.8\n",
        "preference = -0.5\n",
        "#preference = -23"
      ],
      "execution_count": 633,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZpeoHdhHJ_V"
      },
      "source": [
        "Step 3. Training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcHgwCddKF_q"
      },
      "source": [
        "Wiki-wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omBMWjUNHJIO"
      },
      "source": [
        "df = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_tagged.csv\", sep='\\t')\n",
        "#df = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_notags.csv\", sep='\\t')"
      ],
      "execution_count": 580,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdJ8DJVTKHr-"
      },
      "source": [
        "Bts-rnc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk2JIbDTKNMe"
      },
      "source": [
        "df = pd.read_csv(\"./russe-wsi-kit/data/main/bts-rnc/train_bts-rnc_tagged.csv\", sep='\\t')\n",
        "#df = pd.read_csv(\"./russe-wsi-kit/data/main/bts-rnc/train_bts-rnc_notags.csv\", sep='\\t')"
      ],
      "execution_count": 481,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJTndUMKSyU"
      },
      "source": [
        "Active-dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9H6JpHvKbuT"
      },
      "source": [
        "df = pd.read_csv(\"./russe-wsi-kit/data/main/active-dict/train_active-dict_tagged.csv\", sep='\\t')\n",
        "#df = pd.read_csv(\"./russe-wsi-kit/data/main/active-dict/train_active-dict_notags.csv\", sep='\\t')"
      ],
      "execution_count": 634,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV-eOAktUT-X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nPAANE_UVa3"
      },
      "source": [
        "Step 4. Predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQJQb4LHNKn",
        "outputId": "e31b4eae-9a68-486e-9538-107bbec16700"
      },
      "source": [
        "df_pred = predict_clusters_pos(model, df, damping, preference, ninit)\n",
        "#df_pred = predict_clusters_nopos(model, df, damping, preference)"
      ],
      "execution_count": 635,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Now analyzing дар_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing двигатель_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing двойник_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дворец_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing девятка_NOUN ...\n",
            "Predicted clusters: 8\n",
            "Now analyzing дедушка_NOUN ...\n",
            "Empty lexicon in ['быть_VERB']\n",
            "Predicted clusters: 1\n",
            "Now analyzing дежурная_ADJ ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing дежурный_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing декабрист_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing декрет_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing дело_NOUN ...\n",
            "Empty lexicon in []\n",
            "Empty lexicon in []\n",
            "Empty lexicon in ['тут_ADV']\n",
            "Empty lexicon in []\n",
            "Empty lexicon in []\n",
            "Empty lexicon in ['быть_VERB']\n",
            "Empty lexicon in ['как_ADV']\n",
            "Empty lexicon in ['быть_VERB']\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 26\n",
            "Now analyzing демобилизация_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing демократ_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 4\n",
            "Now analyzing демонстрация_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing дерево_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing держава_NOUN ...\n",
            "Empty lexicon in ['зерновая_ADJ']\n",
            "Predicted clusters: 3\n",
            "Now analyzing дерзость_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing десяток_NOUN ...\n",
            "Predicted clusters: 12\n",
            "Now analyzing деятель_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing диалог_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing диаметр_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing диплом_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing директор_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing диск_NOUN ...\n",
            "Empty lexicon in []\n",
            "Empty lexicon in ['грыжа_PROPN', 'межпозвоночный_ADJ']\n",
            "Predicted clusters: 9\n",
            "Now analyzing дичь_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing длина_NOUN ...\n",
            "Predicted clusters: 11\n",
            "Now analyzing доброволец_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing добыча_NOUN ...\n",
            "Predicted clusters: 15\n",
            "Now analyzing доказательство_NOUN ...\n",
            "Predicted clusters: 10\n",
            "Now analyzing доктор_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing долгота_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing доля_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 7\n",
            "Now analyzing дом_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 10\n",
            "Now analyzing дорога_NOUN ...\n",
            "Predicted clusters: 10\n",
            "Now analyzing достижение_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing древесина_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing дупло_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing дура_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дух_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 12\n",
            "Now analyzing дым_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дымок_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing дыхание_NOUN ...\n",
            "Predicted clusters: 17\n",
            "Now analyzing дьявол_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing евро_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 3\n",
            "Now analyzing езда_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing жаворонок_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing жало_VERB ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing жертва_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing жестокость_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing жидкость_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing жить_VERB ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing жилец_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing жир_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing жребий_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing заведение_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing завещание_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing зависимость_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing заголовок_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing заготовка_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing задание_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing задача_NOUN ...\n",
            "Predicted clusters: 11\n",
            "Now analyzing задержка_NOUN ...\n",
            "Predicted clusters: 20\n",
            "Now analyzing зажигалка_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing закон_NOUN ...\n",
            "Predicted clusters: 9\n",
            "Now analyzing закрытие_NOUN ...\n",
            "Predicted clusters: 8\n",
            "Now analyzing заложник_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing замена_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing западня_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing запятая_VERB ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing застой_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing затея_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing затишье_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing затмение_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing затруднение_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 3\n",
            "Now analyzing захоронение_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing звезда_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing звон_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing зеркало_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing зло_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing злоупотребление_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing знак_NOUN ...\n",
            "Empty lexicon in ['«диез»_NOUN']\n",
            "Predicted clusters: 15\n",
            "Now analyzing знамя_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing значение_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing зонт_NOUN ...\n",
            "Predicted clusters: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QPaIkzrJ1bi"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iORpC7CiLdOK"
      },
      "source": [
        "Wiki-wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RGiMytOJx0h",
        "outputId": "228389dd-813a-4633-f7f6-31d29906948d"
      },
      "source": [
        "train = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train.csv\", sep='\\t')\n",
        "for i in range(len(train)):\n",
        "  train.predict_sense_id[i] = df_pred.predict_sense_id[i]\n",
        "train[\"predict_sense_id\"] = train[\"predict_sense_id\"].astype(int)\n",
        "train.to_csv(\"./russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_predicted.csv\", sep='\\t')\n",
        "!python3 russe-wsi-kit/evaluate.py russe-wsi-kit/data/main/wiki-wiki/train_wiki-wiki_predicted.csv"
      ],
      "execution_count": 582,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word\tari\tcount\n",
            "бор\t0.000000\t56\n",
            "замок\t0.880707\t138\n",
            "лук\t0.963609\t110\n",
            "суда\t0.562094\t135\n",
            "\t0.691155\t439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOW7PrkuLJud"
      },
      "source": [
        "Bts-rnc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKCPa46hKtzz",
        "outputId": "0a6b9740-93db-471b-c95a-2810c08206b0"
      },
      "source": [
        "train = pd.read_csv(\"./russe-wsi-kit/data/main/bts-rnc/train.csv\", sep='\\t')\n",
        "for i in range(len(train)):\n",
        "  train.predict_sense_id[i] = df_pred.predict_sense_id[i]\n",
        "train[\"predict_sense_id\"] = train[\"predict_sense_id\"].astype(int)\n",
        "train.to_csv(\"./russe-wsi-kit/data/main/bts-rnc/train_bts-rnc_predicted.csv\", sep='\\t')\n",
        "!python3 russe-wsi-kit/evaluate.py russe-wsi-kit/data/main/bts-rnc/train_bts-rnc_predicted.csv"
      ],
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word\tari\tcount\n",
            "балка\t-0.008102\t119\n",
            "вид\t0.424842\t77\n",
            "винт\t0.037193\t123\n",
            "горн\t0.000000\t51\n",
            "губа\t0.052632\t137\n",
            "жаба\t0.025750\t121\n",
            "клетка\t0.701126\t150\n",
            "крыло\t0.059483\t91\n",
            "купюра\t-0.004951\t150\n",
            "курица\t0.177132\t93\n",
            "лавка\t0.221781\t149\n",
            "лайка\t0.040071\t99\n",
            "лев\t0.000000\t44\n",
            "лира\t0.000000\t49\n",
            "мина\t0.000000\t65\n",
            "мишень\t0.144932\t121\n",
            "обед\t-0.004141\t100\n",
            "оклад\t0.031907\t146\n",
            "опушка\t0.006529\t148\n",
            "полис\t-0.015060\t142\n",
            "пост\t0.467878\t144\n",
            "поток\t0.059190\t136\n",
            "проказа\t0.196820\t146\n",
            "пропасть\t0.118576\t127\n",
            "проспект\t0.002546\t139\n",
            "пытка\t-0.006056\t143\n",
            "рысь\t0.605715\t120\n",
            "среда\t0.323920\t144\n",
            "хвост\t0.372095\t121\n",
            "штамп\t0.240302\t96\n",
            "\t0.153742\t3491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9_3q88ZLeCf"
      },
      "source": [
        "Active-dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gSfbx6vLips",
        "outputId": "b25b467f-07db-4ff9-bb93-061d4bd9be3c"
      },
      "source": [
        "train = pd.read_csv(\"./russe-wsi-kit/data/main/active-dict/train.csv\", sep='\\t')\n",
        "for i in range(len(train)):\n",
        "  train.predict_sense_id[i] = df.predict_sense_id[i]\n",
        "train[\"predict_sense_id\"] = train[\"predict_sense_id\"].astype(int)\n",
        "train.to_csv(\"./russe-wsi-kit/data/main/active-dict/train_active-dict_predicted.csv\", sep='\\t')\n",
        "!python3 russe-wsi-kit/evaluate.py russe-wsi-kit/data/main/active-dict/train_active-dict_predicted.csv"
      ],
      "execution_count": 636,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word\tari\tcount\n",
            "дар\t0.058235\t36\n",
            "двигатель\t0.300000\t15\n",
            "двойник\t0.298016\t25\n",
            "дворец\t0.446247\t13\n",
            "девятка\t0.199738\t47\n",
            "дедушка\t0.000000\t9\n",
            "дежурная\t0.063830\t12\n",
            "дежурный\t-0.022472\t13\n",
            "декабрист\t0.633822\t11\n",
            "декрет\t0.000000\t12\n",
            "дело\t0.099770\t130\n",
            "демобилизация\t0.271647\t14\n",
            "демократ\t0.052883\t18\n",
            "демонстрация\t0.093611\t38\n",
            "дерево\t0.386364\t21\n",
            "держава\t-0.068702\t15\n",
            "дерзость\t0.071848\t37\n",
            "десятка\t0.196124\t36\n",
            "десяток\t0.148955\t21\n",
            "деятель\t0.494700\t14\n",
            "диалог\t0.157618\t14\n",
            "диаметр\t0.298750\t18\n",
            "диплом\t0.216687\t25\n",
            "директор\t0.000000\t11\n",
            "диск\t0.234134\t63\n",
            "дичь\t0.117127\t18\n",
            "длина\t0.163993\t21\n",
            "доброволец\t0.380282\t12\n",
            "добыча\t0.222019\t35\n",
            "доказательство\t0.139439\t24\n",
            "доктор\t0.561778\t17\n",
            "долгота\t0.151020\t13\n",
            "доля\t0.238265\t45\n",
            "дом\t0.300436\t38\n",
            "дорога\t0.283524\t47\n",
            "достижение\t0.203636\t22\n",
            "древесина\t-0.020067\t16\n",
            "дупло\t0.309211\t15\n",
            "дура\t0.435897\t12\n",
            "дух\t0.077768\t77\n",
            "дым\t0.394419\t28\n",
            "дымка\t0.185304\t18\n",
            "дыхание\t0.316065\t56\n",
            "дьявол\t0.196182\t22\n",
            "евро\t0.555556\t8\n",
            "езда\t-0.116430\t14\n",
            "жаворонок\t0.000000\t11\n",
            "жало\t0.037500\t11\n",
            "жертва\t0.241161\t37\n",
            "жестокость\t-0.052670\t14\n",
            "жидкость\t-0.023256\t12\n",
            "жила\t0.331436\t17\n",
            "жилец\t0.203320\t16\n",
            "жир\t0.189723\t15\n",
            "жребий\t0.000000\t15\n",
            "заведение\t0.637631\t14\n",
            "завещание\t-0.016667\t16\n",
            "зависимость\t0.294471\t21\n",
            "заголовок\t0.360229\t22\n",
            "заготовка\t0.140566\t26\n",
            "задание\t0.195446\t33\n",
            "задача\t0.259930\t36\n",
            "задержка\t0.113851\t60\n",
            "зажигалка\t0.446247\t13\n",
            "закон\t0.099852\t56\n",
            "закрытие\t0.398175\t38\n",
            "заложник\t-0.003960\t13\n",
            "замена\t0.028022\t18\n",
            "западня\t0.635762\t11\n",
            "запятая\t0.013015\t14\n",
            "застой\t0.279261\t13\n",
            "затея\t-0.044855\t12\n",
            "затишье\t0.365289\t16\n",
            "затмение\t0.626767\t12\n",
            "затруднение\t0.092873\t15\n",
            "захоронение\t0.136364\t22\n",
            "звезда\t0.188869\t40\n",
            "звон\t-0.115357\t14\n",
            "зеркало\t0.151394\t21\n",
            "зло\t0.154223\t23\n",
            "злоупотребление\t0.690141\t12\n",
            "знак\t0.140679\t55\n",
            "знамя\t0.000000\t14\n",
            "значение\t0.194155\t30\n",
            "зонт\t0.437500\t9\n",
            "\t0.194198\t2073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9VNjZ3ENZS4"
      },
      "source": [
        "### Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZiNhKN25rrv"
      },
      "source": [
        "####Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUET_iVE4NDl"
      },
      "source": [
        "The results presented below are the baseline ones. The baseline parameters are: damping = 0.7, preference = -0.7 (first step clustering) for the first for corpora. For the ru_fasttexts baseline parameters are: damping = 0.7, preference = -10; for navec: damping = 0.7, preference = -23.\n",
        "\n",
        "As you can see for the given solution and parameters for the bts-rnc dataset all corporas give pretty similar results. But for wiki-wiki and active-dict datasets they are much worser.\n",
        "\n",
        "For the further optimizations I decided to stop at the first ruscorpora_upos_skipgram_300_5_2018.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vmjyF8v4Nx7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ_5hMDptVLU"
      },
      "source": [
        "| Corpora/model| Wiki-wiki | Bts-rnc | Active-dict |\n",
        "| --- | --- | --- | --- |\n",
        "| ruscorpora_upos_skipgram_300_5_2018| 0.628726 | 0.134935 | 0.182949 |\n",
        "| taiga_upos_skipgram_300_2_2018| 0.536434 | 0.118117| 0.168383 |\n",
        "| ruwikiruscorpora_upos_skipgram_300_2_2018| 0.510534| 0.112039 | 0.162747 |\n",
        "| news_upos_cbow_600_2_2018| 0.565988 |0.105056 | 0.102181|\n",
        "| ru_fasttext| 0.327494 |0.101594 | 0.157072|\n",
        "| navec | 0.308518 | 0.101310 |0.139624|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ft2jn55zBB"
      },
      "source": [
        "####Best results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFlkoVOr6xqh"
      },
      "source": [
        "To obtain upgraded results not mentioning attempts of trying new corpora, there were tried different hyperparameters of the different combinations of Affinity Propagation, Kmeans ans Spectral Clustering. The best results are preseneted below in the table.\n",
        "\n",
        "The best combinations were the ones of combination of Affinity Propagation at the first step \n",
        "\n",
        "The best params for Affinity Propagation are\n",
        "\n",
        "- wiki-wiki: damping = 0.5, prefrence = -1\n",
        "- bts-rnc: damping = 0.8, preference = -1.1\n",
        "- active-dict: damping = 0.8, preference = -0.5, \n",
        "\n",
        "At the second step best models are KMeans for wiki-wiki and bts-rnc and Affinity Propagation with ninit = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk3g-93054BP"
      },
      "source": [
        "| | Wiki-wiki | Bts-rnc | Active-dict |\n",
        "| --- | --- | --- | --- |\n",
        "| Baseline | 0.628726 | 0.134935 | 0.182949 |\n",
        "| Upgraded | 0.691155 | 0.153742 | 0.206143 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eOZsZT09g_L"
      },
      "source": [
        "#### Discussion\n",
        "\n",
        "Overall, the obtained results are great and there was some imporevement achevied throught the solution. \n",
        "In case there were no NLA course much more could have been done: I have chosen costly data preprocessing,\n",
        "it would be great to remove surnames, numbers and play with the length of the context around target word.\n",
        "\n",
        "Speaking about models, I have thought about trying ELMo vectors, smart parameter tuning via GridSearch,\n",
        "another clustering techniques, and first place idea of this contest(finding similar words to the target words \n",
        "and measuring cosine similarity between context and difference bewteen target word and the most similar term to\n",
        "the target).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2HcgnJI9nNu"
      },
      "source": [
        "###Predict "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZB_gl0e9otm"
      },
      "source": [
        "In the final section I present the prediction made to the test files of the best models obtained in the train part. Actually, if there will be some problems with the submissions on the Codalab I would like these results to be assumed the final ones. It will also be possible to find them in the zip in files: \"wiki-wiki_predicted.csv\", \"bts-rnc_predicted.csv\", \"active-dict_predicted.csv\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVanONSB_NNJ"
      },
      "source": [
        "#### Wiki-wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1f-dnsA_9Pt"
      },
      "source": [
        "modelfile = \"drive/My Drive/Corpora/ruscorpora_upos_skipgram_300_5_2018.vec.gz\"\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(modelfile, binary=False)"
      ],
      "execution_count": 471,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMC40jjv_PiY",
        "outputId": "7f63c8f7-d821-4d57-aacf-3078b77930e7"
      },
      "source": [
        "damping = 0.5\n",
        "preference = -1\n",
        "ninit = 20\n",
        "\n",
        "df = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/test_wiki-wiki_tagged.csv\", sep='\\t')\n",
        "\n",
        "df_pred = predict_clusters_pos(model, df, damping, preference, ninit)\n",
        "\n",
        "test = pd.read_csv(\"./russe-wsi-kit/data/main/wiki-wiki/test.csv\", sep='\\t')\n",
        "for i in range(len(test)):\n",
        "  test.predict_sense_id[i] = df_pred.predict_sense_id[i]\n",
        "test[\"predict_sense_id\"] = test[\"predict_sense_id\"].astype(int)\n",
        "test.to_csv(\"./russe-wsi-kit/data/main/wiki-wiki/wiki-wiki_predicted.csv\", sep='\\t')\n"
      ],
      "execution_count": 639,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Now analyzing банк_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing белок_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing бит_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing горе_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing граната_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing граф_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing душа_NOUN ...\n",
            "Predicted clusters: 1\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reBZYH6jDPjA"
      },
      "source": [
        "Bts-rnc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aew-gAQYDi89",
        "outputId": "8b6735bb-f04b-4406-a418-7b5e535bfa3b"
      },
      "source": [
        "damping = 0.8\n",
        "preference = -1.1\n",
        "ninit = 20\n",
        "\n",
        "df = pd.read_csv(\"./russe-wsi-kit/data/main/bts-rnc/test_bts-rnc_tagged.csv\", sep='\\t')\n",
        "\n",
        "df_pred = predict_clusters_pos(model, df, damping, preference, ninit)\n",
        "\n",
        "test = pd.read_csv(\"./russe-wsi-kit/data/main/bts-rnc/test.csv\", sep='\\t')\n",
        "for i in range(len(test)):\n",
        "  test.predict_sense_id[i] = df_pred.predict_sense_id[i]\n",
        "test[\"predict_sense_id\"] = test[\"predict_sense_id\"].astype(int)\n",
        "test.to_csv(\"./russe-wsi-kit/data/main/bts-rnc/bts-rnc_predicted.csv\", sep='\\t')"
      ],
      "execution_count": 640,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Now analyzing акция_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing баба_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing байка_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing бум_INTJ ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing бычок_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing вал_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing газ_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing гвоздик_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing гипербола_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing град_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing гусеница_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing дождь_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing домино_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing забой_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing икра_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing кабачок_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing капот_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing карьер_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing кличка_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing ключ_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing кок_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing кольцо_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing концерт_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing котелок_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing крона_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing крупа_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing кулак_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing лейка_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing лук_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing мандарин_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing ножка_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing опора_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing патрон_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing печать_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing пол_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing полоз_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing почерк_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing пробка_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing рак_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing рок_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing свет_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing секрет_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing скат_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing слог_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing стан_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing стопка_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing таз_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing такса_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing тюрьма_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing шах_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing шашка_NOUN ...\n",
            "Predicted clusters: 2\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55JWpxMrDoFQ"
      },
      "source": [
        "Active-dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPvdewOyDtMJ",
        "outputId": "50360643-8048-4d58-df63-2b26df3ae2c2"
      },
      "source": [
        "damping = 0.8\n",
        "preference = -0.5\n",
        "ninit = 25\n",
        "\n",
        "df = pd.read_csv(\"./russe-wsi-kit/data/main/active-dict/test_active-dict_tagged.csv\", sep='\\t')\n",
        "\n",
        "df_pred = predict_clusters_pos(model, df, damping, preference, ninit)\n",
        "\n",
        "test = pd.read_csv(\"./russe-wsi-kit/data/main/active-dict/test.csv\", sep='\\t')\n",
        "for i in range(len(test)):\n",
        "  test.predict_sense_id[i] = df_pred.predict_sense_id[i]\n",
        "test[\"predict_sense_id\"] = test[\"predict_sense_id\"].astype(int)\n",
        "test.to_csv(\"./russe-wsi-kit/data/main/active-dict/active-dict_predicted.csv\", sep='\\t')"
      ],
      "execution_count": 642,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Now analyzing давление_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'он_PRON', 'всегда_ADV', 'на_ADP']\n",
            "Predicted clusters: 3\n",
            "Now analyzing дама_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing данные_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing дата_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing двойка_NOUN ...\n",
            "Predicted clusters: 9\n",
            "Now analyzing двор_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing дворник_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing девка_NOUN ...\n",
            "Predicted clusters: 15\n",
            "Now analyzing девочка_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing девушка_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'ты_PRON', 'быть_VERB']\n",
            "Predicted clusters: 5\n",
            "Now analyzing девчонка_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing дед_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing дезертир_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing действие_NOUN ...\n",
            "Empty lexicon in ['между_ADP']\n",
            "Predicted clusters: 14\n",
            "Now analyzing действительность_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing декларация_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing декорация_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing делегат_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing деление_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing дельфин_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing демократия_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing день_NOUN ...\n",
            "Predicted clusters: 20\n",
            "Now analyzing деньги_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing деревня_NOUN ...\n",
            "Empty lexicon in ['ну_PART', 'ты_PRON']\n",
            "Predicted clusters: 7\n",
            "Now analyzing десант_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing десерт_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing деталь_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing детектив_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing ребенок_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing деятельность_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing диагональ_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing диво_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing диета_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дикарь_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing диктатор_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дипломатия_NOUN ...\n",
            "Empty lexicon in ['тут_ADV', 'не_PART', 'до_ADP']\n",
            "Predicted clusters: 1\n",
            "Now analyzing дискант_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing дистанция_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing дитя_NOUN ...\n",
            "Empty lexicon in ['какой_DET', 'же_PART', 'ты_PRON', 'все-таки_PART']\n",
            "Predicted clusters: 9\n",
            "Now analyzing дневник_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing днище_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing дно_NOUN ...\n",
            "Predicted clusters: 11\n",
            "Now analyzing добро_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing добродетель_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing доверие_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing дождь_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing доза_NOUN ...\n",
            "Predicted clusters: 8\n",
            "Now analyzing доклад_NOUN ...\n",
            "Empty lexicon in ['с_ADP']\n",
            "Predicted clusters: 4\n",
            "Now analyzing документ_NOUN ...\n",
            "Predicted clusters: 10\n",
            "Now analyzing долг_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing должник_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing доллар_NOUN ...\n",
            "Empty lexicon in []\n",
            "Predicted clusters: 2\n",
            "Now analyzing дополнение_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing дорожка_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing досада_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing доска_NOUN ...\n",
            "Predicted clusters: 10\n",
            "Now analyzing достаток_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing достоинство_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing достояние_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing доступ_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing дочь_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing дощечка_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing драгоценность_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing драма_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing дробь_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing дрова_NOUN ...\n",
            "Empty lexicon in ['нарубать_VERB']\n",
            "Predicted clusters: 2\n",
            "Now analyzing дрожжи_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing дрожь_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дружба_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing дрянь_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing дуб_NOUN ...\n",
            "Empty lexicon in ['вот_PART']\n",
            "Predicted clusters: 5\n",
            "Now analyzing дубина_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing дуга_NOUN ...\n",
            "Empty lexicon in ['начерчивать_VERB']\n",
            "Empty lexicon in ['автосцепка_NOUN']\n",
            "Predicted clusters: 8\n",
            "Now analyzing дурак_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing душ_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing душа_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'она_PRON', 'нет_PART']\n",
            "Predicted clusters: 8\n",
            "Now analyzing дуэт_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing дыня_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дыра_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing дядя_NOUN ...\n",
            "Empty lexicon in ['какой-то_DET']\n",
            "Empty lexicon in ['ты_PRON', 'что_SCONJ']\n",
            "Predicted clusters: 7\n",
            "Now analyzing дятел_NOUN ...\n",
            "Empty lexicon in ['среди_ADP', 'мы_PRON', 'быть_VERB']\n",
            "Predicted clusters: 2\n",
            "Now analyzing еврей_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing еда_NOUN ...\n",
            "Empty lexicon in ['есть_VERB']\n",
            "Predicted clusters: 3\n",
            "Now analyzing единица_NOUN ...\n",
            "Predicted clusters: 19\n",
            "Now analyzing единство_NOUN ...\n",
            "Predicted clusters: 10\n",
            "Now analyzing ежевика_NOUN ...\n",
            "Empty lexicon in ['заросли_NOUN']\n",
            "Predicted clusters: 4\n",
            "Now analyzing ель_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing ельник_NOUN ...\n",
            "Empty lexicon in ['нарубать_VERB']\n",
            "Predicted clusters: 5\n",
            "Now analyzing ерунда_NOUN ...\n",
            "Empty lexicon in ['что_PRON', 'за_ADP']\n",
            "Predicted clusters: 7\n",
            "Now analyzing жадность_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing жажда_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing жалоба_NOUN ...\n",
            "Empty lexicon in ['какой_DET', 'у_ADP', 'вы_PRON']\n",
            "Predicted clusters: 5\n",
            "Now analyzing жалость_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing жар_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing железо_NOUN ...\n",
            "Predicted clusters: 8\n",
            "Now analyzing жемчуг_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing жена_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing жених_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing жест_NOUN ...\n",
            "Empty lexicon in ['протестующий_ADJ']\n",
            "Predicted clusters: 5\n",
            "Now analyzing жизнь_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'ты_PRON', 'свой_DET', 'у_ADP', 'он_PRON', 'свой_DET']\n",
            "Predicted clusters: 10\n",
            "Now analyzing жилка_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing жук_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing журавль_NOUN ...\n",
            "Empty lexicon in ['над_ADP', 'колодцемскрип_NOUN']\n",
            "Predicted clusters: 3\n",
            "Now analyzing журнал_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing жуть_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing забота_NOUN ...\n",
            "Empty lexicon in ['это_PRON', 'не_PART', 'ваш_DET']\n",
            "Predicted clusters: 8\n",
            "Now analyzing завеса_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing завивка_NOUN ...\n",
            "Empty lexicon in ['ее_DET', 'растрепываться_VERB']\n",
            "Predicted clusters: 2\n",
            "Now analyzing завиток_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing завоевание_NOUN ...\n",
            "Predicted clusters: 8\n",
            "Now analyzing завтрак_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'мы_PRON', 'с_NOUN', 'до_NOUN']\n",
            "Predicted clusters: 2\n",
            "Now analyzing загадка_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing зад_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing зазор_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing заимствование_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing заказ_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing закат_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing закуска_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing зал_NOUN ...\n",
            "Predicted clusters: 9\n",
            "Now analyzing залог_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing замазка_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing заметка_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing замечание_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing замок_NOUN ...\n",
            "Predicted clusters: 9\n",
            "Now analyzing занавес_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing занятие_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing запас_NOUN ...\n",
            "Predicted clusters: 9\n",
            "Now analyzing записка_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing запись_NOUN ...\n",
            "Empty lexicon in ['переслушивать_VERB', 'информант_NOUN']\n",
            "Predicted clusters: 6\n",
            "Now analyzing заповедь_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing запрос_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing запруда_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing запуск_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing заработок_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing заражение_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing зараза_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing зародыш_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing заря_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing заряд_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing зарядка_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'кто_PRON', 'быть_VERB', 'для_ADP']\n",
            "Predicted clusters: 11\n",
            "Now analyzing засада_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing защита_NOUN ...\n",
            "Empty lexicon in ['я_PRON', 'ты_PRON', 'не_PART']\n",
            "Predicted clusters: 26\n",
            "Now analyzing защитник_NOUN ...\n",
            "Predicted clusters: 13\n",
            "Now analyzing заявка_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing заявление_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing заяц_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing звание_NOUN ...\n",
            "Empty lexicon in ['город-герой_NOUN']\n",
            "Predicted clusters: 7\n",
            "Now analyzing звено_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing зверство_NOUN ...\n",
            "Empty lexicon in ['ради_ADP']\n",
            "Predicted clusters: 4\n",
            "Now analyzing зверь_NOUN ...\n",
            "Empty lexicon in ['мы_PRON', 'что_SCONJ', 'какой-нибудь_DET']\n",
            "Predicted clusters: 3\n",
            "Now analyzing звонок_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing звук_NOUN ...\n",
            "Predicted clusters: 9\n",
            "Now analyzing здоровье_NOUN ...\n",
            "Empty lexicon in ['как_ADV']\n",
            "Predicted clusters: 6\n",
            "Now analyzing зелень_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing земля_NOUN ...\n",
            "Empty lexicon in ['у_ADP', 'мы_PRON', 'много_ADV']\n",
            "Predicted clusters: 11\n",
            "Now analyzing зенит_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing зерно_NOUN ...\n",
            "Predicted clusters: 15\n",
            "Now analyzing злодей_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing змея_NOUN ...\n",
            "Predicted clusters: 2\n",
            "Now analyzing знакомство_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing знание_NOUN ...\n",
            "Predicted clusters: 6\n",
            "Now analyzing значок_NOUN ...\n",
            "Predicted clusters: 4\n",
            "Now analyzing золото_NOUN ...\n",
            "Empty lexicon in ['быть_VERB', 'на_ADP']\n",
            "Predicted clusters: 9\n",
            "Now analyzing зона_NOUN ...\n",
            "Predicted clusters: 5\n",
            "Now analyzing зонтик_NOUN ...\n",
            "Predicted clusters: 3\n",
            "Now analyzing зоология_NOUN ...\n",
            "Predicted clusters: 7\n",
            "Now analyzing зубр_NOUN ...\n",
            "Predicted clusters: 1\n",
            "Now analyzing зуд_NOUN ...\n",
            "Predicted clusters: 3\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAukWl8J9o2e"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}
